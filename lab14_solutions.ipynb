{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab14_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ6UU3Ay8vdZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Introduction\n",
        "\n",
        "In this lab we will explore neural style transfer in artwork!\n",
        "\n",
        "This lab is adapted from Section 8.3 of Chollet.  The code is also available at https://keras.io/examples/neural_style_transfer/.\n",
        "\n",
        "# Set Runtime Type\n",
        "\n",
        "This lab will take literally forever if you don't set your runtime type to GPU or TPU, so go ahead and do that now :)\n",
        "\n",
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyD5alyF8gzw",
        "colab_type": "code",
        "outputId": "f8c83402-0621-47b9-b018-6387a9ffb242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm53u7K9gcdq",
        "colab_type": "text"
      },
      "source": [
        "# Set up google drive\n",
        "\n",
        "To apply neural style transfer, we need a content image and a style image.  Of course our content image will be a picture of Benedict.  Our style image will be a painting by Georgia O'Keeffe.  (You can also experiment with other content and style images if you prefer!)\n",
        "\n",
        "As usual, I have shared the necessary files with you in a google drive folder.  To get the data into colab, do these steps:\n",
        "\n",
        "1. Sign into drive.google.com\n",
        "2. Click on \"Shared with me\" on the left side of the screen\n",
        "3. Right click on the stat344ne_style_transfer folder and select \"Add Shortcut to Drive\"\n",
        "4. Run the code cell below and click on the link that is displayed.  It will pop up a new browser tab where you have to authorize Colab to access your google drive.  Then, copy the sequence of numbers and letters that is displayed and paste it in the space that shows up in the code cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Zj475ehdiC",
        "colab_type": "code",
        "outputId": "ef5668a7-73e9-4792-a2f3-d68391c93e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzhNg4EDI-MM",
        "colab_type": "text"
      },
      "source": [
        "# Utility functions to deal with images:\n",
        "\n",
        "You don't need to modify these functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaV0127uJH7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# util function to open, resize and format pictures into appropriate tensors\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel.  This is a transformation that was done\n",
        "    # by VGG19, so we need to replicate it here.\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'.  Again, dealing with encoding used by VGG19\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo0b3__iJBiD",
        "colab_type": "text"
      },
      "source": [
        "# Read in Images\n",
        "\n",
        "The code below reads in the content and style images using the utility functions defined above.  The results are numpy arrays.\n",
        "\n",
        "**Please set your name in the first line below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvSBSXC5JgdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_name = 'evan'\n",
        "\n",
        "content_image_path = '/content/drive/My Drive/stat344ne_style_transfer/benedict.jpg'\n",
        "style_image_path = '/content/drive/My Drive/stat344ne_style_transfer/okeeffe.jpg'\n",
        "#style_image_path = '/content/drive/My Drive/stat344ne_style_transfer/scream.jpg'\n",
        "result_prefix = '/content/drive/My Drive/stat344ne_style_transfer/result_' + my_name + '_'\n",
        "\n",
        "# dimensions of the generated picture.\n",
        "width, height = load_img(content_image_path).size\n",
        "n_h = 400\n",
        "n_w = int(width * n_h / height)\n",
        "\n",
        "# get tensor representations of our images\n",
        "content_image_np = preprocess_image(content_image_path)\n",
        "style_image_np = preprocess_image(style_image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ITVQ5UXRhE",
        "colab_type": "text"
      },
      "source": [
        "# Individual Loss Functions\n",
        "\n",
        "We'll define functions to calculate all the contributions to the loss for neural style transfer.\n",
        "\n",
        "#### 1. Gram matrix\n",
        "\n",
        "Write a function below to calculate the Gram matrix.  Recall that the Gram matrix is obtained in two stages:\n",
        "\n",
        " * Find an \"unfurled\" features matrix `F`.  In this matrix, each row should correspond to a channel and the columns are the vectorized rows and columns put together.  There are many ways to achieve this.  Here is one:\n",
        "    * Reshape `x` to have the same number of channels it currently has, and put all remaining dimensions together.  You will want to use `K.reshape`.  One trick is that if you use a shape that includes (-1, 5), that dimension of the shape will be a product of all remaining dimensions.  For example, if you input was of shape (2, 3, 5) then after reshaping to (-1, 5) your final array will have shape (2*3, 5). \n",
        "    * Transpose the result of the previous step.  You will want to use `K.transpose`.\n",
        " * Compute the Gram matrix as the dot product of `F` and its transpose.  You will want to use `K.dot` and `K.transpose`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNPjHctBMr5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(x):\n",
        "    '''\n",
        "    Calculate the Gram matrix summary of style\n",
        "\n",
        "    Arguments:\n",
        "     - x: a Keras backend tensor\n",
        "    \n",
        "    Returns:\n",
        "     - Gram matrix\n",
        "    '''\n",
        "    F = K.transpose(K.reshape(x, (-1, np.shape(x)[-1])))\n",
        "    gram = K.dot(F, K.transpose(F))\n",
        "    return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkSQffTFdj4Z",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Style Loss\n",
        "\n",
        "If $G^S$ and $G^X$ are the Gram matrices for the style and combination (X) images respectively, then the style loss is\n",
        "\n",
        "$$\\frac{1}{4 n_h^2 * n_w^2 * n_c^2} \\sum_{i=1}^{n_c} \\sum_{j=1}^{n_c} (G^S_{i,j} - G^X_{i,j})^2$$\n",
        "\n",
        "You will want to use `K.square` and `K.sum`.  The division by the leading constants can be done with the usual python arithmetic operators.  Remember that in python, $4^2$ is calculated as `4**2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af27aEghdmVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_loss(a_style, a_combination):\n",
        "    '''\n",
        "    Calculate style loss\n",
        "\n",
        "    Arguments:\n",
        "     - style_gram: gram matrix for style image (S) representation\n",
        "     - combination_gram: gram matrix for combination image (X) representation\n",
        "    \n",
        "    Return:\n",
        "     - style loss\n",
        "    '''\n",
        "    # extract width, height, and number of channels in this layer's activations\n",
        "    n_h, n_w, n_c = K.int_shape(a_style)\n",
        "\n",
        "    # create Gram matrices for style and combination images using gram_matrix\n",
        "    style_gram = gram_matrix(a_style)\n",
        "    combination_gram = gram_matrix(a_combination)\n",
        "\n",
        "    # calculate loss\n",
        "    loss = K.sum(K.square(style_gram - combination_gram)) / (4.0 * (n_c**2) * (n_w**2) * (n_h**2))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ29n2Bhg6vp",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Content Loss\n",
        "\n",
        "If $a^C$ and $a^X$ are the activation outputs for the content image and combination image respectively, then the content loss is\n",
        "\n",
        "$$0.5\\sum_{i = 1}^{n_h} \\sum_{j = 1}^{n_w} \\sum_{c=1}^{n_c} (a^C_{i,j,c} - a^X_{i,j,c})^2$$\n",
        "\n",
        "You will want to use the functions `K.square` and `K.sum`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8kNzU8Pg9sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def content_loss(a_content, a_combination):\n",
        "    '''\n",
        "    Calculate content loss\n",
        "\n",
        "    Arguments:\n",
        "     - a_content: activation output from given layer for content image\n",
        "     - a_combination: activation output from given layer for combination image\n",
        "\n",
        "    Reurns:\n",
        "     - content loss\n",
        "    '''\n",
        "    return 0.5 * K.sum(K.square(a_combination - a_content))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxRVmhzQjW_9",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Total Variation Loss\n",
        "We will use the version of total variation loss in our book.  If `x` is the combination image, the total variation loss is\n",
        "\n",
        "$$\\sum_{i=1}^{n_h-1} \\sum_{j=1}^{n_w-1} \\sum_{c = 1}^{n_c}\\left\\{ (x_{i,j,c} - x_{i+1,j,c})^2  + (x_{i,j,c} - x_{i,j+1,c})^2\\right\\}^{1.25}$$\n",
        "\n",
        "We will organize this calculation into five steps:\n",
        "\n",
        "1. Create an $(n_h - 1) \\times (n_w - 1)$ array `a` with the differences between elements in adjacent rows and the same column.\n",
        "2. Create an $(n_h - 1) \\times (n_w - 1)$ array `b` with the differences between elements in adjacent columns and the same row.\n",
        "3. Calculate $a^2 + b^2$, where the squares are elementwise\n",
        "4. Raise each element of the array created in step 3 to the power of 1.25\n",
        "5. Add all the numbers in the array from step 4\n",
        "\n",
        "The indexing for steps 1 and 2 is a little tricky so I'll give you that.  It's worth thinking through what's happening with them though; this trick has come in useful for me many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDjFQQisjjKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_variation_loss(x, n_h, n_w):\n",
        "    '''\n",
        "    Calculate total variation loss\n",
        "\n",
        "    Arguments:\n",
        "     - x: a Keras backend tensor representing the combination image.\n",
        "       It has shape (1, n_h, n_w, 3)\n",
        "     - n_h, n_w: height and width of x\n",
        "    \n",
        "    Returns:\n",
        "     - total variation loss\n",
        "    '''\n",
        "    a = x[:,:n_h-1,:n_w-1,:] - x[:,1:,:n_w-1,:]\n",
        "    b = x[:,:n_h-1,:n_w-1,:] - x[:,:n_h-1,1:,:]\n",
        "    step_3 = K.square(a) + K.square(b)\n",
        "    step_4 = K.pow(step_3, 1.25)\n",
        "    step_5 = K.sum(step_4)\n",
        "\n",
        "    return step_5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-ZSes4SXGWM",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Assemble network inputs\n",
        "\n",
        "We will need to obtain activation outputs from the network for the content, style, and combination images.  The easiest way to do this is to assemble them into a single tensor of shape $(3, n_h, n_w, 3)$, where the first 3 is because we have 3 images and the last 3 is for the three channels of the RGB representation of images.  To do this, we'll:\n",
        "\n",
        "1. Create tensor `variable`s with the content and style images.  These images are currently in the numpy arrays `content_image_np` and `style_image_np`.\n",
        "2. Create a tensor `placeholder` for the combination image.\n",
        "3. Concatenate the three items above into a single array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtz_lPsYNk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create content and style images using K.variable\n",
        "content_image = K.variable(content_image_np)\n",
        "style_image = K.variable(style_image_np)\n",
        "\n",
        "# Create combination image using K.placeholder with shape (1, n_h, n_w, 3)\n",
        "combination_image = K.placeholder((1, n_h, n_w, 3))\n",
        "\n",
        "# Concatenate the tensors above using K.concatenate\n",
        "# For the code below to work, please use the order\n",
        "# [content_image, style_image, combination_image]\n",
        "input_tensor = K.concatenate([content_image,\n",
        "                              style_image,\n",
        "                              combination_image], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J2-QDtfuJxp",
        "colab_type": "text"
      },
      "source": [
        "#### Model Set Up (nothing for you to do)\n",
        "\n",
        "The code below reads in the VGG19 model fit and creates a dictionary with the layer names and activation outputs for each layer in the VGG19 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHHQyJDuaO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCJmLQv8uj6S",
        "colab_type": "text"
      },
      "source": [
        "Here's a look at the dictionary of layer output activations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vmJOtG9ufaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f00dfcbc-536c-43c9-a269-719f5e056a97"
      },
      "source": [
        "outputs_dict"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'block1_conv1': <tf.Tensor 'block1_conv1_21/Relu:0' shape=(3, 400, 299, 64) dtype=float32>,\n",
              " 'block1_conv2': <tf.Tensor 'block1_conv2_21/Relu:0' shape=(3, 400, 299, 64) dtype=float32>,\n",
              " 'block1_pool': <tf.Tensor 'block1_pool_21/MaxPool:0' shape=(3, 200, 149, 64) dtype=float32>,\n",
              " 'block2_conv1': <tf.Tensor 'block2_conv1_21/Relu:0' shape=(3, 200, 149, 128) dtype=float32>,\n",
              " 'block2_conv2': <tf.Tensor 'block2_conv2_21/Relu:0' shape=(3, 200, 149, 128) dtype=float32>,\n",
              " 'block2_pool': <tf.Tensor 'block2_pool_21/MaxPool:0' shape=(3, 100, 74, 128) dtype=float32>,\n",
              " 'block3_conv1': <tf.Tensor 'block3_conv1_21/Relu:0' shape=(3, 100, 74, 256) dtype=float32>,\n",
              " 'block3_conv2': <tf.Tensor 'block3_conv2_21/Relu:0' shape=(3, 100, 74, 256) dtype=float32>,\n",
              " 'block3_conv3': <tf.Tensor 'block3_conv3_21/Relu:0' shape=(3, 100, 74, 256) dtype=float32>,\n",
              " 'block3_conv4': <tf.Tensor 'block3_conv4_21/Relu:0' shape=(3, 100, 74, 256) dtype=float32>,\n",
              " 'block3_pool': <tf.Tensor 'block3_pool_21/MaxPool:0' shape=(3, 50, 37, 256) dtype=float32>,\n",
              " 'block4_conv1': <tf.Tensor 'block4_conv1_21/Relu:0' shape=(3, 50, 37, 512) dtype=float32>,\n",
              " 'block4_conv2': <tf.Tensor 'block4_conv2_21/Relu:0' shape=(3, 50, 37, 512) dtype=float32>,\n",
              " 'block4_conv3': <tf.Tensor 'block4_conv3_21/Relu:0' shape=(3, 50, 37, 512) dtype=float32>,\n",
              " 'block4_conv4': <tf.Tensor 'block4_conv4_21/Relu:0' shape=(3, 50, 37, 512) dtype=float32>,\n",
              " 'block4_pool': <tf.Tensor 'block4_pool_21/MaxPool:0' shape=(3, 25, 18, 512) dtype=float32>,\n",
              " 'block5_conv1': <tf.Tensor 'block5_conv1_21/Relu:0' shape=(3, 25, 18, 512) dtype=float32>,\n",
              " 'block5_conv2': <tf.Tensor 'block5_conv2_21/Relu:0' shape=(3, 25, 18, 512) dtype=float32>,\n",
              " 'block5_conv3': <tf.Tensor 'block5_conv3_21/Relu:0' shape=(3, 25, 18, 512) dtype=float32>,\n",
              " 'block5_conv4': <tf.Tensor 'block5_conv4_21/Relu:0' shape=(3, 25, 18, 512) dtype=float32>,\n",
              " 'block5_pool': <tf.Tensor 'block5_pool_21/MaxPool:0' shape=(3, 12, 9, 512) dtype=float32>,\n",
              " 'input_19': <tf.Tensor 'concat_19:0' shape=(3, 400, 299, 3) dtype=float32>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeRghUhUuszO",
        "colab_type": "text"
      },
      "source": [
        "#### 6. Assemble the final combined loss (a scalar tensor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQ3xmFm7Jm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are the weights of the different loss components\n",
        "total_variation_weight = 1.0\n",
        "style_weight = 0.2\n",
        "content_weight = 0.025\n",
        "\n",
        "# Define a Keras tensor variable initialized with the value 0.0 using K.variable\n",
        "loss = K.variable(0.0)\n",
        "\n",
        "# Extract activation features for the layer used to define content loss\n",
        "# You don't need to edit these 3 lines\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "content_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "\n",
        "# calculate the content loss and store in the variable cl\n",
        "# You will need to call the content_loss function you defined in part 3 above,\n",
        "# providing content_image_features and combination_features as arguments\n",
        "cl = content_loss(content_image_features, combination_features) # call content_loss\n",
        "\n",
        "# Add the content_weight * cl to the loss.\n",
        "loss = loss + content_weight * cl\n",
        "\n",
        "# Add the style_weight * style_loss to the loss for each layer used for style\n",
        "# feature_layers is a list of layers that are used for measuring style\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    # Extract activation features for the layer used to define style loss\n",
        "    # You don't need to edit these 3 lines\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "\n",
        "    # calculate the style loss and store in the variable sl\n",
        "    # you will need to call the style_loss function you defined in part 2 above\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "\n",
        "    # Add the style_weight * sl to the loss\n",
        "    loss = loss + style_weight * sl\n",
        "\n",
        "# calculate the total variation loss based on the combination image using the\n",
        "# total_variation_loss function you defined in part 4 above\n",
        "tvl = total_variation_loss(combination_image, n_h, n_w)\n",
        "\n",
        "# add total_variation_weight * tvl to the loss\n",
        "loss = loss + total_variation_weight * tvl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_namFQuCxPKk",
        "colab_type": "text"
      },
      "source": [
        "#### 7. Create a function of the combination_image that returns the gradient of the loss with respect to the pixel values in the combination image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJAJ3swHu_y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the gradients of the loss with respect to the combination image\n",
        "# You will need to use K.gradients\n",
        "grads = K.gradients(loss, [combination_image])\n",
        "\n",
        "# define a function that takes the combination image as an input and returns the\n",
        "# grads.  You will need to use K.function.  Note that grads is already a list so\n",
        "# you don't need to put it in another list.\n",
        "f_grad = K.function([combination_image], grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nYikzlxuU_",
        "colab_type": "text"
      },
      "source": [
        "#### 8. Perform estimation by gradient descent\n",
        "\n",
        "This will take a while to run, go get a cup of coffee :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwnOc1DevCJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "434aab31-959c-49c5-fc94-2375a06d58f3"
      },
      "source": [
        "# intialize optimization at the original content image\n",
        "x = preprocess_image(content_image_path)\n",
        "\n",
        "# We'll use a learning rate of 0.00005\n",
        "alpha = 0.00005\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(6001):\n",
        "    # Calculate the gradient vector by calling f_grad\n",
        "    # Remember that you'll need to extract component 0 of the list it returns\n",
        "    grad_value = f_grad([x])[0]\n",
        "\n",
        "    # Perform a gradient descent update step on x with step size alpha\n",
        "    x = x - alpha * grad_value\n",
        "\n",
        "    # every 10 iterations, save current generated image\n",
        "    # you don't need to change this code\n",
        "    if i % 500 == 0:\n",
        "        img = deprocess_image(x.copy())\n",
        "        fname = result_prefix + 'at_iteration_%d.png' % i\n",
        "        save_img(fname, img)\n",
        "        end_time = time.time()\n",
        "        print('Iteration', i, 'image saved as', fname)\n",
        "        print('Iterations completed in %ds' % (end_time - start_time))\n",
        "        start_time = time.time()\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_0.png\n",
            "Iterations completed in 1s\n",
            "Iteration 500 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_500.png\n",
            "Iterations completed in 37s\n",
            "Iteration 1000 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_1000.png\n",
            "Iterations completed in 36s\n",
            "Iteration 1500 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_1500.png\n",
            "Iterations completed in 36s\n",
            "Iteration 2000 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_2000.png\n",
            "Iterations completed in 37s\n",
            "Iteration 2500 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_2500.png\n",
            "Iterations completed in 36s\n",
            "Iteration 3000 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_3000.png\n",
            "Iterations completed in 36s\n",
            "Iteration 3500 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_3500.png\n",
            "Iterations completed in 36s\n",
            "Iteration 4000 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_4000.png\n",
            "Iterations completed in 36s\n",
            "Iteration 4500 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_4500.png\n",
            "Iterations completed in 36s\n",
            "Iteration 5000 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_5000.png\n",
            "Iterations completed in 36s\n",
            "Iteration 5500 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_5500.png\n",
            "Iterations completed in 37s\n",
            "Iteration 6000 image saved as /content/drive/My Drive/stat344ne_style_transfer/result_evan_at_iteration_6000.png\n",
            "Iterations completed in 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlk_PITDyj8K",
        "colab_type": "text"
      },
      "source": [
        "Congratulations on finishing a long lab!  Check out your results in the google drive folder!\n",
        "\n",
        "If you want, you can go back to the beginning and uncomment the line changing the style image to the painting Scream by Edvard Munch instead of the O'Keeffe painting under the \"Read in Images\" heading."
      ]
    }
  ]
}